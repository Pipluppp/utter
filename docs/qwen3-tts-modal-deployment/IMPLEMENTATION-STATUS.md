# Qwen3-TTS Modal Deployment - Implementation Status

> **Last Updated**: 2026-02-02
> **Status**: Fully Deployed â€” 1.7B (A10G) + 0.6B (T4), both using SDPA

---

## Summary

The Qwen3-TTS 1.7B and 0.6B voice cloning services have been deployed to Modal.com and fully integrated into the Utter web application. Both deployments use SDPA attention (standardized after benchmarking showed it outperforms Flash Attention 2).

---

## Deployment Status

| Component | Status | Date | Notes |
|-----------|--------|------|-------|
| Modal CLI Setup | âœ… Complete | 2026-01-27 | Using `uv run modal` |
| HuggingFace Secret | âœ… Complete | 2026-01-27 | `huggingface-secret` |
| Modal Volume | âœ… Complete | 2026-01-27 | `qwen3-tts-models` |
| 1.7B Model Download | âœ… Complete | 2026-01-27 | 4.23 GB |
| 1.7B Service (SDPA) | âœ… **Deployed** | 2026-01-27 | A10G GPU, production |
| 0.6B Model Download | âœ… Complete | 2026-01-28 | 2.34 GB |
| 0.6B Service (SDPA) | âœ… **Deployed** | 2026-02-02 | T4 GPU, cost-optimized |
| Utter Integration | âœ… Complete | 2026-01-28 | Backend + frontend done |
| FA2 Benchmark | âœ… Complete | 2026-02-01 | SDPA 18-22% faster |
| FA2 Deployment | ðŸ›‘ **Stopped** | 2026-02-02 | Removed after benchmarking |
| 1.7B vs 0.6B Benchmark | âœ… Complete | 2026-02-02 | See results below |
| Voice Design Model | Skipped | | Not needed for MVP |

---

## Benchmark Results (2026-02-02)

### 1.7B vs 0.6B Model Comparison

| Model | GPU | Cold Start | Short (56 chars) | Medium (800 chars) |
|-------|-----|------------|------------------|-------------------|
| **Qwen3-TTS-12Hz-1.7B-Base** | NVIDIA A10G | 108s | **14.6s** | **113s** |
| **Qwen3-TTS-12Hz-0.6B-Base** | Tesla T4 | **43s** | 17.4s | 176s |

**Key Findings:**
- 0.6B has 2.5x faster cold start
- 1.7B is 20-56% faster for generation
- Choose based on workload pattern

### SDPA vs FA2 (2026-02-01)

| Metric | SDPA | FA2 | Winner |
|--------|------|-----|--------|
| Cold Start | 68s | 83s | SDPA (22% faster) |
| Long Text | 5.5 min | 6.5 min | SDPA (18% faster) |

---

## Live Endpoints

### Production: 1.7B on A10G (SDPA)

| Endpoint | URL | Method |
|----------|-----|--------|
| Clone | `https://duncab013--qwen3-tts-voice-clone-qwen3ttsservice-clone.modal.run` | POST |
| Clone Batch | `https://duncab013--qwen3-tts-voice-clone-qwen3ttsservice-clone-batch.modal.run` | POST |
| Health | `https://duncab013--qwen3-tts-voice-clone-qwen3ttsservice-health.modal.run` | GET |
| Languages | `https://duncab013--qwen3-tts-voice-clone-qwen3ttsservice-languages.modal.run` | GET |

**Modal Dashboard**: https://modal.com/apps/duncab013/main/deployed/qwen3-tts-voice-clone

### Cost-Optimized: 0.6B on T4 (SDPA)

| Endpoint | URL | Method |
|----------|-----|--------|
| Clone | `https://duncab013--qwen3-tts-voice-clone-06b-qwen3ttsservice-clone.modal.run` | POST |
| Health | `https://duncab013--qwen3-tts-voice-clone-06b-qwen3ttsservice-health.modal.run` | GET |
| Languages | `https://duncab013--qwen3-tts-voice-clone-06b-qwen3ttsservice-languages.modal.run` | GET |

**Modal Dashboard**: https://modal.com/apps/duncab013/main/deployed/qwen3-tts-voice-clone-06b

### Model Comparison

| Property | 0.6B (T4) | 1.7B (A10G) |
|----------|-----------|-------------|
| GPU | Tesla T4 (16 GB) | NVIDIA A10G (24 GB) |
| GPU Cost | ~$0.59/hr | ~$1.10/hr |
| Cold start | ~43s | ~108s |
| Short text (56 chars) | ~17s | ~15s |
| Medium text (800 chars) | ~176s | ~113s |
| Warm (short text) | N/A | ~10.5s | ~11.1s |
| Warm (long text) | N/A | ~35.6s | ~39.7s |
| Attention | SDPA | SDPA | flash_attention_2 |
| PyTorch | 2.10 | 2.10 | 2.9 (pinned) |
| Model size on volume | 2.34 GB | 4.23 GB | 4.23 GB |

### FA2 vs SDPA Benchmark Summary â€” Final Results

Comprehensive benchmarking completed 2026-02-02:

| Test | SDPA | FA2 | Winner |
|------|------|-----|--------|
| Cold Start | 68s | 83s | **SDPA (22% faster)** |
| Short text (~50 chars) | ~10s | ~10s | Tie |
| Medium text (~200 chars) | ~30s | ~27s | Mixed |
| Long text (~500 chars) | ~85s | ~67s | Mixed |
| Very long text (2600 chars) | **337s (5.5min)** | 393s (6.5min) | **SDPA (18% faster)** |

**Key Finding**: Looking at actual Modal.com GPU execution times (not just API response), SDPA is consistently 18% faster than FA2 for TTS workloads.

**Decision**: Standardize on SDPA for all deployments. FA2 variant will be stopped.

See [FA2-BENCHMARK-REPORT.md](./optimization/FA2-BENCHMARK-REPORT.md) for full details.

---

## Implementation Files

```
modal_app/qwen3_tts/
â”œâ”€â”€ __init__.py               # Package initialization (v1.0.0)
â”œâ”€â”€ app.py                    # 1.7B Modal app (A10G GPU, SDPA)
â”œâ”€â”€ app_fa2.py                # 1.7B Modal app (A10G GPU, Flash Attention 2)
â”œâ”€â”€ app_06b.py                # 0.6B Modal app (T4 GPU)
â”œâ”€â”€ config.py                 # Configuration constants
â”œâ”€â”€ download_models.py        # Model download script for Modal volume
â””â”€â”€ test_client.py            # API test client (generic, takes --endpoint)

test/
â”œâ”€â”€ test_qwen3_tts.py         # Test script (supports --model 1.7B/1.7B-FA2/0.6B)
â”œâ”€â”€ compare_fa2_sdpa.py       # SDPA vs FA2 latency comparison script
â”œâ”€â”€ reference/
â”‚   â”œâ”€â”€ audio.wav             # Reference audio (multi-sentence)
â”‚   â””â”€â”€ audio_text.txt        # Transcript (14 sentences)
â””â”€â”€ outputs/
    â”œâ”€â”€ 1.7B/
    â”‚   â””â”€â”€ clone_output.wav  # Generated output from 1.7B model
    â”œâ”€â”€ 0.6B/
    â”‚   â””â”€â”€ clone_output.wav  # Generated output from 0.6B model
    â”œâ”€â”€ SDPA/                  # FA2 comparison outputs
    â”‚   â”œâ”€â”€ cold_start.wav
    â”‚   â”œâ”€â”€ short.wav
    â”‚   â”œâ”€â”€ medium.wav
    â”‚   â””â”€â”€ long.wav
    â”œâ”€â”€ FA2/                   # FA2 comparison outputs
    â”‚   â”œâ”€â”€ cold_start.wav
    â”‚   â”œâ”€â”€ short.wav
    â”‚   â”œâ”€â”€ medium.wav
    â”‚   â””â”€â”€ long.wav
    â””â”€â”€ comparison_results.json # Detailed timing data

backend/                        # Utter integration files (modified/new)
â”œâ”€â”€ .env                        # TTS_PROVIDER=qwen, QWEN_MODAL_ENDPOINT
â”œâ”€â”€ config.py                   # Added TTS_PROVIDER, SUPPORTED_LANGUAGES
â”œâ”€â”€ models.py                   # Added reference_transcript, language columns
â”œâ”€â”€ database.py                 # Added ALTER TABLE auto-migration
â”œâ”€â”€ main.py                     # Updated clone/generate, added /api/languages
â”œâ”€â”€ requirements.txt            # Added httpx>=0.27.0
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ tts.py                  # Rewritten as provider router
â”‚   â””â”€â”€ tts_qwen.py            # NEW â€” async Qwen3-TTS Modal client
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ clone.html              # Added transcript textarea + language
â”‚   â””â”€â”€ generate.html           # Added language dropdown
â””â”€â”€ static/js/
    â””â”€â”€ app.js                  # Updated forms for transcript + language
```

---

## Pain Points & Lessons Learned

### 1. FastAPI Import Error on Local Machine

**Problem**: When running `modal deploy app.py`, the local Python environment didn't have FastAPI installed, causing `ModuleNotFoundError: No module named 'fastapi'`.

**Root Cause**: Modal parses the Python file locally before deploying. If you use top-level imports for packages only installed in the container image, it fails.

**Solution**: Use lazy imports inside methods, not at module level:
```python
# BAD - fails on local machine
from fastapi import HTTPException
from pydantic import BaseModel

# GOOD - import inside methods
@modal.fastapi_endpoint(docs=True, method="POST")
def clone(self, request: dict):
    from fastapi import HTTPException
    from fastapi.responses import StreamingResponse
    # ...
```

**Lesson for 0.6B/Voice Design**: Keep top-level imports minimal. Only import `modal` and standard library at module level.

---

### 2. Unicode/UTF-8 Encoding Error on Windows

**Problem**: Running `modal run` or `modal deploy` on Windows failed with:
```
'charmap' codec can't encode character '\u2713' in position 0
```

**Root Cause**: Modal CLI outputs Unicode checkmarks (âœ“) which Windows cmd.exe can't display by default.

**Solution**: Pipe output through `cat` or set environment variable:
```bash
# Option 1: Pipe through cat
PYTHONIOENCODING=utf-8 uv run modal deploy app.py 2>&1 | cat

# Option 2: Just pipe it
uv run modal deploy app.py 2>&1 | cat
```

**Lesson for 0.6B/Voice Design**: Always use the piped command pattern on Windows.

---

### 3. Reference Audio URL Returns 403 Forbidden

**Problem**: The Qwen sample audio URL from their documentation returns HTTP 403:
```
https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/clone.wav
```

**Root Cause**: The Alibaba Cloud OSS storage may have regional restrictions or require specific headers.

**Solution**: Use base64-encoded audio instead of URLs. Added `_resolve_ref_audio()` method to handle both:
```python
def _resolve_ref_audio(self, ref_audio: str) -> str:
    if ref_audio.startswith(("http://", "https://")):
        return ref_audio  # URL - pass directly

    # Base64 - decode and save to temp file
    audio_bytes = base64.b64decode(ref_audio)
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        f.write(audio_bytes)
        return f.name
```

**Lesson for 0.6B/Voice Design**: Always support base64 input. Don't rely on external URLs for testing.

---

### 4. "File name too long" Error with Base64

**Problem**: Passing base64 data directly to `model.generate_voice_clone()` caused:
```
[Errno 36] File name too long: 'UklGRsy6BgBXQVZFZm10...'
```

**Root Cause**: The qwen-tts library treats the `ref_audio` parameter as a file path, not raw data. When given a long base64 string, it tries to open it as a filename.

**Solution**: The `_resolve_ref_audio()` method decodes base64 and saves to a temp file before passing to the model. Temp files are cleaned up in a `finally` block.

**Lesson for 0.6B/Voice Design**: The qwen-tts library only accepts URLs or file paths for reference audio, not raw bytes or base64.

---

### 5. Container Caching After Redeployment

**Problem**: After redeploying with code fixes, the old code was still running because Modal containers were cached.

**Solution**: Stop the app before redeploying to force fresh containers:
```bash
uv run modal app stop qwen3-tts-voice-clone
uv run modal deploy app.py
```

Or wait for the 5-minute idle timeout to expire.

**Lesson for 0.6B/Voice Design**: After code changes, explicitly stop the app before redeploying for immediate effect.

---

### 6. Cold Start Time (~90 seconds)

**Problem**: First request after container spindown takes ~90-95 seconds.

**Breakdown**:
- Container provisioning: ~5-10s
- Image layer download: ~10-20s (cached after first deploy)
- Model loading from volume: ~30-60s
- First inference: ~5-10s

**Mitigation**:
- Container idle timeout set to 300s (5 minutes)
- Models pre-cached in volume (avoids ~5-10 minute HuggingFace download)
- Send periodic health checks to keep warm

**Lesson for 0.6B/Voice Design**: Cold start is unavoidable but predictable. Document expected times for users.

---

## Test Reference Files

Located in `test/reference/`:

**audio.wav**: Multi-sentence reference audio for voice cloning

**audio_text.txt** (transcript):
```
Prosecutors have opened a massive investigation into allegations of fixing games and illegal betting.
Different telescope designs perform differently and have different strengths and weaknesses.
We can continue to strengthen the education of good lawyers.
Feedback must be timely and accurate throughout the project.
Humans also judge distance by using the relative sizes of objects.
Churches should not encourage it or make it look harmless.
Learn about setting up wireless network configuration.
You can eat them fresh, cooked or fermented.
If this is true then those who tend to think creatively really are somehow different.
She will likely jump for joy and want to skip straight to the honeymoon.
The sugar syrup should create very fine strands of sugar that drape over the handles.
But really in the grand scheme of things, this information is insignificant.
I let the positive overrule the negative.
He wiped his brow with his forearm.
```

---

## Testing Commands

### Health Check
```bash
curl https://duncab013--qwen3-tts-voice-clone-qwen3ttsservice-health.modal.run
```

### Clone with Test Reference (using Python)
```python
import base64
import json
import requests

# Read reference audio
with open('test/reference/audio.wav', 'rb') as f:
    audio_b64 = base64.b64encode(f.read()).decode('utf-8')

# Read transcript
with open('test/reference/audio_text.txt', 'r') as f:
    ref_text = f.read().strip()

# Make request
response = requests.post(
    'https://duncab013--qwen3-tts-voice-clone-qwen3ttsservice-clone.modal.run',
    json={
        'text': 'Hello, this is a test of voice cloning.',
        'language': 'English',
        'ref_audio_base64': audio_b64,
        'ref_text': ref_text,
    },
    timeout=120
)

# Save output
with open('output.wav', 'wb') as f:
    f.write(response.content)
```

---

## Configuration Reference

### Model Configuration (config.py)
```python
MODEL_1_7B = "Qwen/Qwen3-TTS-12Hz-1.7B-Base"
MODEL_0_6B = "Qwen/Qwen3-TTS-12Hz-0.6B-Base"
TOKENIZER_ID = "Qwen/Qwen3-TTS-Tokenizer-12Hz"

GPU_CONFIG = {
    "0.6B": "T4",      # 16GB VRAM
    "1.7B": "A10G",    # 24GB VRAM
}

CONTAINER_IDLE_TIMEOUT = 300  # 5 minutes
MAX_CONCURRENT_INPUTS = 10
REQUEST_TIMEOUT = 300  # 5 minutes
```

### Volume Contents
```
/vol/models/
â”œâ”€â”€ Qwen3-TTS-12Hz-1.7B-Base/    # 40 files, 4.23 GB
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors        # 3.68 GB
â”‚   â”œâ”€â”€ speech_tokenizer/        # 650 MB
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Qwen3-TTS-12Hz-0.6B-Base/    # 40 files, 2.34 GB
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors        # 1.74 GB
â”‚   â”œâ”€â”€ speech_tokenizer/        # 650 MB
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Qwen3-TTS-Tokenizer-12Hz/    # 19 files, 0.64 GB
â””â”€â”€ huggingface/                 # Cache directory
```

---

## Next Steps

See [NEXT-TASKS.md](./NEXT-TASKS.md) for detailed planning and prompting guidance.

### Task 1: 0.6B Deployment - COMPLETE

Deployed 2026-01-28. T4 GPU, ~32s cold start, same API as 1.7B.

### Task 2: Voice Design Model - SKIPPED

Skipping for now to focus on core Utter integration.

### Task 3: Utter Backend Integration - COMPLETE

Completed 2026-01-28. Full backend and frontend integration of Qwen3-TTS into the Utter web application.

**What was built:**

| Component | File(s) | Description |
|-----------|---------|-------------|
| Config | `backend/config.py`, `backend/.env` | `TTS_PROVIDER`, `QWEN_MODAL_ENDPOINT`, `SUPPORTED_LANGUAGES` |
| DB schema | `backend/models.py`, `backend/database.py` | Added `reference_transcript`, `language` columns + auto-migration |
| Qwen TTS service | `backend/services/tts_qwen.py` (NEW) | Async httpx client, base64 audio, calls Modal endpoint |
| TTS router | `backend/services/tts.py` | Dispatches to Echo or Qwen based on `TTS_PROVIDER` |
| API endpoints | `backend/main.py` | Updated `/api/clone`, `/api/generate`; added `/api/languages` |
| Clone frontend | `backend/templates/clone.html`, `app.js` | Transcript textarea, language dropdown, validation |
| Generate frontend | `backend/templates/generate.html`, `app.js` | Language dropdown, elapsed timer, no chunking |
| Dependencies | `backend/requirements.txt` | Added `httpx>=0.27.0` |

---

## Changelog

### 2026-02-02
- **Modal Timeout Fix**:
  - Increased function timeout from 300s to 900s (15 minutes) in `app.py` and `app_fa2.py`
  - Required for very long text generation (2000+ characters)
  - Updated test runner default timeout to match
- **Very Long Text Benchmark** (`test/scripts/run_comparison.py --text long`):
  - Input: 2594 characters (tokenization tutorial)
  - SDPA: 337.17s (5.5min actual GPU time)
  - FA2: 392.87s (6.5min actual GPU time)
  - **SDPA is 18% faster** â€” decisive result from actual Modal execution times
- **Final Decision: Standardize on SDPA**:
  - SDPA outperforms FA2 for TTS workloads (18% faster, 22% faster cold start)
  - FA2 optimizations don't apply to short TTS sequences (~500-2000 tokens)
  - PyTorch 2.10's native SDPA is highly optimized
  - Simpler deployment (no flash-attn wheel, no torch version pinning)
- **Deployment Plan**:
  - Keep: `qwen3-tts-voice-clone` (1.7B SDPA) â€” Production
  - Stop: `qwen3-tts-voice-clone-fa2` (1.7B FA2) â€” Free endpoints
  - Redeploy: `qwen3-tts-voice-clone-06b` (0.6B SDPA) â€” Lighter workloads
- **Documentation Updates**:
  - Updated `FA2-BENCHMARK-REPORT.md` with Test 3 results and final recommendations
  - Updated `README.md` with benchmark conclusions
  - Updated `IMPLEMENTATION-STATUS.md` with deployment plan
  - Updated `test/README.md` with expected performance data
- **Generated Outputs**:
  - `test/outputs/SDPA/long.wav` (7.68 MB, ~2.8 min audio)
  - `test/outputs/FA2/long.wav` (7.73 MB, ~2.8 min audio)
  - `test/results/comparison_20260202_000334.json`

### 2026-02-01
- **Flash Attention 2 Deployment** (Task from `flash-attention-optimization-plan.md`):
  - Updated `modal_app/qwen3_tts/app_fa2.py` with pre-built wheel approach (no compilation needed)
  - Pinned torch 2.9.0 to match available flash-attn wheel (v2.8.3)
  - Used pre-built wheel from GitHub releases (253.8 MB, cxx11abiTRUE variant)
  - Deployed to Modal as `qwen3-tts-voice-clone-fa2`
  - Stopped 0.6B deployment to free up web endpoints (8 endpoint limit on free tier)
  - Health check verified: `attention_implementation: "flash_attention_2"`
- **Latency Benchmark â€” Test 1** (`test/compare_fa2_sdpa.py`):
  - Tested combined clone+generate performance
  - Cold start: SDPA 68.4s vs FA2 83.3s (FA2 22% slower)
  - Warm short (39 chars): SDPA 10.49s vs FA2 11.11s
  - Warm medium (97 chars): SDPA 17.88s vs FA2 18.81s
  - Warm long (255 chars): SDPA 35.61s vs FA2 39.67s
  - Saved to `test/outputs/comparison_results.json`
- **Latency Benchmark â€” Test 2** (`test/compare_generation_only.py`):
  - Tested generation-focused performance with 4 text lengths
  - Tiny (12 chars): FA2 14% faster (6.76s vs 7.88s)
  - Short (44 chars): FA2 13% faster (11.10s vs 12.82s)
  - Medium (187 chars): FA2 13% faster (27.34s vs 31.52s)
  - Long (506 chars): FA2 23% faster (66.82s vs 86.48s)
  - Note: Results noisy due to Modal container restarts
  - Saved to `test/outputs/generation_benchmark_results.json`
- **Audio Output Files Generated**:
  - `test/outputs/SDPA/` â€” cold_start.wav, short.wav, medium.wav, long.wav
  - `test/outputs/FA2/` â€” cold_start.wav, short.wav, medium.wav, long.wav
- **New Test Scripts**:
  - `test/compare_fa2_sdpa.py` â€” Combined clone+generate benchmark
  - `test/compare_generation_only.py` â€” Generation-focused benchmark with throughput metrics
- **Documentation Updates**:
  - Created `FA2-BENCHMARK-REPORT.md` â€” Comprehensive report with all raw timing data
  - Updated `flash-attention-optimization-plan.md` â€” Added benchmark results section
  - Updated `IMPLEMENTATION-STATUS.md` â€” Added FA2 endpoints and comparison tables
  - Updated `test_qwen3_tts.py` â€” Added `--model 1.7B-FA2` support
- **Key Finding**: FA2 shows 13-23% speedup for generation (longer text = more benefit), but cold start is 22% slower. Recommend SDPA for production, FA2 for batch/long-text scenarios.

### 2026-01-28
- Deployed 0.6B model to Modal (T4 GPU, `app_06b.py`)
- Downloaded 0.6B model to shared volume (2.34 GB)
- Tested 0.6B clone endpoint with local reference audio
- Restructured `test/` directory: `reference/`, `outputs/1.7B/`, `outputs/0.6B/`
- Updated `test_qwen3_tts.py` to support `--model 1.7B/0.6B` flag
- **Utter Backend Integration** (Task 3):
  - Added `TTS_PROVIDER` / `QWEN_MODAL_ENDPOINT` config with `.env` support
  - Added `reference_transcript` and `language` columns to Voice/Generation models
  - Created `backend/services/tts_qwen.py` â€” async httpx client for Modal endpoint
  - Rewrote `backend/services/tts.py` as provider router (Echo vs Qwen)
  - Updated `/api/clone` to accept transcript + language form fields
  - Updated `/api/generate` to pass ref_text + language to TTS service
  - Added `GET /api/languages` endpoint
  - Updated `clone.html` with transcript textarea + language dropdown
  - Updated `generate.html` with language dropdown
  - Updated `app.js` for new form fields, validation, and elapsed time counter
  - Added `httpx>=0.27.0` to `requirements.txt`
  - Verified app boots cleanly with `uv run uvicorn main:app`

### 2026-01-27
- Initial deployment of 1.7B model
- Added base64 audio support with temp file handling
- Fixed FastAPI import issues for Modal compatibility
- Documented all pain points and solutions
- Created test reference files structure
