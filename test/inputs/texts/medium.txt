This tutorial covers the process of tokenization in large language models. Tokenization is one of the less enjoyable aspects of working with large language models, but it’s necessary to understand in detail. The process is complex, with many hidden pitfalls to be aware of. Much of the odd behavior in large language models can be traced back to tokenization. Tokenization was covered in my earlier guide, Let’s build GPT from scratch, but using a naive, simple version. The accompanying notebook demonstrates loading the Shakespeare dataset as a training set. This dataset is simply a large Python string containing text. The core question becomes: how do we feed text into large language models? In that simple example, we created a vocabulary of 65 possible characters that appeared in the string.